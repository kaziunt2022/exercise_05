{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **The fifth in-class-exercise (40 points in total, 11/11/2021)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(20 points) The purpose of the question is to practice different machine learning algorithms for text classification as well as the performance evaluation. In addition, you are requried to conduct *10 fold cross validation (https://scikit-learn.org/stable/modules/cross_validation.html)* in the training. \n",
    "\n",
    "The dataset can be download from here: https://github.com/unt-iialab/info5731_spring2021/blob/main/class_exercises/exercise09_datacollection.zip. The dataset contains two files train data and test data for sentiment analysis in IMDB review, it has two categories: 1 represents positive and 0 represents negative. You need to split the training data into training and validate data (80% for training and 20% for validation, https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6) and perform 10 fold cross validation while training the classifier. The final trained model was final evaluated on the test data. \n",
    "\n",
    "Algorithms:\n",
    "\n",
    "(1) MultinominalNB\n",
    "\n",
    "(2) SVM \n",
    "\n",
    "(3) KNN \n",
    "\n",
    "(4) Decision tree\n",
    "\n",
    "(5) Random Forest\n",
    "\n",
    "(6) XGBoost\n",
    "\n",
    "Evaluation measurement:\n",
    "\n",
    "(1) Accuracy\n",
    "\n",
    "(2) Recall\n",
    "\n",
    "(3) Precison \n",
    "\n",
    "(4) F-1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "import re, string, nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "import gensim\n",
    "from scipy.cluster import hierarchy\n",
    "#nltk.download(\"stopwords\")\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "with open(\"stsa-train.txt\", 'r') as file: # open the txt file \n",
    "    data = file.read().splitlines()\n",
    "sentiments= [int(review[0]) for review in data] # extract sentiments from txt file\n",
    "reviews = [review[2:] for review in data] # extract reviews from the txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_list = stopwords.words('english') # importing stopwords\n",
    "punctuations_list = string.punctuation # get punctuations\n",
    "tokenizer = nltk.tokenize.TweetTokenizer() # initiliza tokenizer\n",
    "lemmatizer = WordNetLemmatizer() # initialize word lemmatizer\n",
    "def preprocessing(text):\n",
    "    \"\"\"\n",
    "    This function will clean the givern text\n",
    "    \"\"\"\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    text = re.sub('[\\W]+', ' ', text.lower())\n",
    "    text = text + \" \".join(emoticons).replace('-', '')\n",
    "    tokenize_text = [lemmatizer.lemmatize(word.lower()) for word in nltk.tokenize.word_tokenize(text) if (word not in stopwords_list) and (word not in punctuations_list) and (len(word)>=2) and (word.isalnum())]\n",
    "    return \" \".join(tokenize_text)\n",
    "reviews = list(map(preprocessing, reviews)) # Clean all the reviews got from txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(reviews, sentiments, test_size=0.2) # split the data as 80% train, 20% validation\n",
    "c_vectorize=CountVectorizer()\n",
    "X_train_reviews = c_vectorize.fit_transform(X_train) # transform the data in vector formate\n",
    "X_test_reviews = c_vectorize.transform(X_test) # transform the data in vector formate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation \n",
    "\"\"\"\n",
    "Algorithms:\n",
    "\n",
    "(1) MultinominalNB\n",
    "\n",
    "(2) SVM \n",
    "\n",
    "(3) KNN \n",
    "\n",
    "(4) Decision tree\n",
    "\n",
    "(5) Random Forest\n",
    "\n",
    "(6) XGBoost\n",
    "\"\"\"\n",
    "# make models of upper given algorithms with train data (respectively)\n",
    "multinomial_model = MultinomialNB().fit(X_train_reviews, y_train)  #(1)\n",
    "svm_model = LinearSVC().fit(X_train_reviews, y_train)  #(2)\n",
    "knn_model = KNeighborsClassifier().fit(X_train_reviews, y_train)  #(3)\n",
    "d_tree_model = DecisionTreeClassifier().fit(X_train_reviews, y_train)  #(4)\n",
    "r_forest_model = RandomForestClassifier().fit(X_train_reviews, y_train)  #(5)\n",
    "x_g_boost_model = GradientBoostingClassifier().fit(X_train_reviews, y_train)  #(6)\n",
    "# print out score of model withrespect to validation data (respectively)\n",
    "print(\"Score of 'MultinominalNB' for training and validationg data: \", multinomial_model.score(X_test_reviews, y_test))\n",
    "print(\"Score of 'SVM' for training and validationg data: \", svm_model.score(X_test_reviews, y_test))\n",
    "print(\"Score of 'KNN' for training and validationg data: \", knn_model.score(X_test_reviews, y_test))\n",
    "print(\"Score of 'Decision tree' for training and validationg data: \", d_tree_model.score(X_test_reviews, y_test))\n",
    "print(\"Score of 'Random Forest' for training and validationg data: \", r_forest_model.score(X_test_reviews, y_test))\n",
    "print(\"Score of 'XGBoost' for training and validationg data: \", x_g_boost_model.score(X_test_reviews, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = [\"accuracy\", 'recall', 'precision', 'f1'] # parameters of evaluations\n",
    "X = c_vectorize.fit_transform(reviews) # get vectorized form of reviews\n",
    "y = sentiments \n",
    "# Apply cross validationn to each above mentioed algorithms with k-fold=10\n",
    "mulit_nomial_scores = cross_validate(MultinomialNB(), X, y, scoring=scoring, cv=10)\n",
    "svm_score = cross_validate(LinearSVC(), X, y, scoring=scoring, cv=10)\n",
    "knn_score = cross_validate(KNeighborsClassifier(), X, y, scoring=scoring, cv=10)\n",
    "d_tree_score = cross_validate(DecisionTreeClassifier(), X, y, scoring=scoring, cv=10)\n",
    "r_forest_score = cross_validate(RandomForestClassifier(), X, y, scoring=scoring, cv=10)\n",
    "xg_boost_score = cross_validate(GradientBoostingClassifier(), X, y, scoring=scoring, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the eveluations\n",
    "print(\"-------- MultinominalNB Cross Validation to Validations Set --------\")\n",
    "print(\"Accuracy:\\t\", mulit_nomial_scores[\"test_accuracy\"].mean())\n",
    "print(\"Recall:\\t\", mulit_nomial_scores[\"test_recall\"].mean())\n",
    "print(\"Precesion:\\t\", mulit_nomial_scores[\"test_precision\"].mean())\n",
    "print('F-1:\\t', mulit_nomial_scores[\"test_f1\"].mean())\n",
    "print()\n",
    "print(\"-------- SVM Cross Validation to Validations Set --------\")\n",
    "print(\"Accuracy:\\t\", svm_score[\"test_accuracy\"].mean())\n",
    "print(\"Recall:\\t\", svm_score[\"test_recall\"].mean())\n",
    "print(\"Precesion:\\t\", svm_score[\"test_precision\"].mean())\n",
    "print('F-1:\\t', svm_score[\"test_f1\"].mean())\n",
    "print()\n",
    "print(\"-------- KNN Cross Validation to Validations Set --------\")\n",
    "print(\"Accuracy:\\t\", knn_score[\"test_accuracy\"].mean())\n",
    "print(\"Recall:\\t\", knn_score[\"test_recall\"].mean())\n",
    "print(\"Precesion:\\t\", knn_score[\"test_precision\"].mean())\n",
    "print('F-1:\\t', knn_score[\"test_f1\"].mean())\n",
    "print()\n",
    "print(\"-------- Decision tree Cross Validation to Validations Set --------\")\n",
    "print(\"Accuracy:\\t\", d_tree_score[\"test_accuracy\"].mean())\n",
    "print(\"Recall:\\t\", d_tree_score[\"test_recall\"].mean())\n",
    "print(\"Precesion:\\t\", d_tree_score[\"test_precision\"].mean())\n",
    "print('F-1:\\t', d_tree_score[\"test_f1\"].mean())\n",
    "print()\n",
    "print(\"-------- Random Forest Cross Validation to Validations Set --------\")\n",
    "print(\"Accuracy:\\t\", r_forest_score[\"test_accuracy\"].mean())\n",
    "print(\"Recall:\\t\", r_forest_score[\"test_recall\"].mean())\n",
    "print(\"Precesion:\\t\", r_forest_score[\"test_precision\"].mean())\n",
    "print('F-1:\\t', r_forest_score[\"test_f1\"].mean())\n",
    "print()\n",
    "print(\"-------- XGBoost Cross Validation to Validations Set --------\")\n",
    "print(\"Accuracy:\\t\", xg_boost_score[\"test_accuracy\"].mean())\n",
    "print(\"Recall:\\t\", xg_boost_score[\"test_recall\"].mean())\n",
    "print(\"Precesion:\\t\", xg_boost_score[\"test_precision\"].mean())\n",
    "print('F-1:\\t', xg_boost_score[\"test_f1\"].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "with open(\"stsa-test.txt\", 'r') as file: # open the test file\n",
    "    data = file.read().splitlines()\n",
    "test_sentiments= [int(review[0]) for review in data] # extract sentiments\n",
    "test_reviews = [review[2:] for review in data] # extrct test reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_reviews = list(map(preprocessing, test_reviews)) # clean the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = c_vectorize.fit_transform(reviews) # make vector of reviews\n",
    "y = sentiments\n",
    "X_test_d = c_vectorize.transform(test_reviews) # make vector of test reviews\n",
    "y_test_d = test_sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make models of given algortihms respectively,  with training reviews and sentiments\n",
    "multinomial_model = MultinomialNB().fit(X, y)\n",
    "svm_model = LinearSVC().fit(X, y)\n",
    "knn_model = KNeighborsClassifier().fit(X, y)\n",
    "d_tree_model = DecisionTreeClassifier().fit(X, y)\n",
    "r_forest_model = RandomForestClassifier().fit(X, y)\n",
    "x_g_boost_model = GradientBoostingClassifier().fit(X, y)\n",
    "# print out the score of models with respect to test reviews and sentiments\n",
    "print(\"Score of 'MultinominalNB' for training and test data: \", multinomial_model.score(X_test_d, y_test_d))\n",
    "print(\"Score of 'SVM' for training and test data: \", svm_model.score(X_test_d, y_test_d))\n",
    "print(\"Score of 'KNN' for training and test data: \", knn_model.score(X_test_d, y_test_d))\n",
    "print(\"Score of 'Decision tree' for training and test data: \", d_tree_model.score(X_test_d, y_test_d))\n",
    "print(\"Score of 'Random Forest' for training and test data: \", r_forest_model.score(X_test_d, y_test_d))\n",
    "print(\"Score of 'XGBoost' for training and test data: \", x_g_boost_model.score(X_test_d, y_test_d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = [\"accuracy\", 'recall', 'precision', 'f1'] # evaluating criteria\n",
    "# Apply cross validationn to each above mentioed algorithms with k-fold=10\n",
    "mulit_nomial_scores = cross_validate(multinomial_model, X_test_d, y_test_d, scoring=scoring, cv=10)\n",
    "svm_score = cross_validate(svm_model, X_test_d, y_test_d, scoring=scoring, cv=10)\n",
    "knn_score = cross_validate(knn_model, X_test_d, y_test_d, scoring=scoring, cv=10)\n",
    "d_tree_score = cross_validate(d_tree_model, X_test_d, y_test_d, scoring=scoring, cv=10)\n",
    "r_forest_score = cross_validate(r_forest_model, X_test_d, y_test_d, scoring=scoring, cv=10)\n",
    "X_test_dg_boost_score = cross_validate(x_g_boost_model, X_test_d, y_test_d, scoring=scoring, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out cross validation scores\n",
    "print(\"-------- MultinominalNB Cross Validation to test data --------\")\n",
    "print(\"Accuracy:\\t\", mulit_nomial_scores[\"test_accuracy\"].mean())\n",
    "print(\"Recall:\\t\", mulit_nomial_scores[\"test_recall\"].mean())\n",
    "print(\"Precesion:\\t\", mulit_nomial_scores[\"test_precision\"].mean())\n",
    "print('F-1:\\t', mulit_nomial_scores[\"test_f1\"].mean())\n",
    "print()\n",
    "print(\"-------- SVM Cross Validation to test data --------\")\n",
    "print(\"Accuracy:\\t\", svm_score[\"test_accuracy\"].mean())\n",
    "print(\"Recall:\\t\", svm_score[\"test_recall\"].mean())\n",
    "print(\"Precesion:\\t\", svm_score[\"test_precision\"].mean())\n",
    "print('F-1:\\t', svm_score[\"test_f1\"].mean())\n",
    "print()\n",
    "print(\"-------- KNN Cross Validation to test data --------\")\n",
    "print(\"Accuracy:\\t\", knn_score[\"test_accuracy\"].mean())\n",
    "print(\"Recall:\\t\", knn_score[\"test_recall\"].mean())\n",
    "print(\"Precesion:\\t\", knn_score[\"test_precision\"].mean())\n",
    "print('F-1:\\t', knn_score[\"test_f1\"].mean())\n",
    "print()\n",
    "print(\"-------- Decision tree Cross Validation to test data --------\")\n",
    "print(\"Accuracy:\\t\", d_tree_score[\"test_accuracy\"].mean())\n",
    "print(\"Recall:\\t\", d_tree_score[\"test_recall\"].mean())\n",
    "print(\"Precesion:\\t\", d_tree_score[\"test_precision\"].mean())\n",
    "print('F-1:\\t', d_tree_score[\"test_f1\"].mean())\n",
    "print()\n",
    "print(\"-------- Random Forest Cross Validation to test data --------\")\n",
    "print(\"Accuracy:\\t\", r_forest_score[\"test_accuracy\"].mean())\n",
    "print(\"Recall:\\t\", r_forest_score[\"test_recall\"].mean())\n",
    "print(\"Precesion:\\t\", r_forest_score[\"test_precision\"].mean())\n",
    "print('F-1:\\t', r_forest_score[\"test_f1\"].mean())\n",
    "print()\n",
    "print(\"-------- XGBoost Cross Validation to test data --------\")\n",
    "print(\"Accuracy:\\t\", xg_boost_score[\"test_accuracy\"].mean())\n",
    "print(\"Recall:\\t\", xg_boost_score[\"test_recall\"].mean())\n",
    "print(\"Precesion:\\t\", xg_boost_score[\"test_precision\"].mean())\n",
    "print('F-1:\\t', xg_boost_score[\"test_f1\"].mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(20 points) The purpose of the question is to practice different machine learning algorithms for text clustering\n",
    "Please downlad the dataset by using the following link.  https://www.kaggle.com/PromptCloudHQ/amazon-reviews-unlocked-mobile-phones\n",
    "(You can also use different text data which you want)\n",
    "\n",
    "Apply the listed clustering methods to the dataset:\n",
    "\n",
    "K means, \n",
    "DBSCAN,\n",
    "Hierarchical clustering. \n",
    "\n",
    "You can refer to of the codes from  the follwing link below. \n",
    "https://www.kaggle.com/karthik3890/text-clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write your code here.\n",
    "df = pd.read_csv(\"Amazon_Unlocked_Mobile.csv\", usecols=[\"Reviews\"], nrows=2000) # import the data as pandas dataframe\n",
    "df['Reviews'].replace('', np.nan, inplace=True) # replace empty rows with NAN\n",
    "df.dropna(subset=['Reviews'], inplace=True) # remove rows having NAN\n",
    "amazon_reviews = list(df['Reviews'])  # get reviews in list formate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_reviews = list(map(preprocessing, amazon_reviews)) # clean the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()\n",
    "bow = count_vect.fit_transform(amazon_reviews) # convert reviews in vector formate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans(n_clusters = 10,init='k-means++',random_state=99) # Inititate k-means model \n",
    "model.fit(bow) # apply k0means model to reviews\n",
    "cluster_labels = model.labels_ # get cluster labels\n",
    "df['K-mean Clusters'] = cluster_labels # save cluster labels in dataframe\n",
    "cluster_center=model.cluster_centers_ # get cluster center\n",
    "\n",
    "s_score = silhouette_score(bow, cluster_labels) # get silhouette score\n",
    "print(\"Silhouette_score of K-means clustering: \", s_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out top 10 clusters with their top 10 keywords \n",
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = count_vect.get_feature_names_out()\n",
    "for i in range(10): print(\"Cluster -->\" , i+1, '-->', [terms[ind] for ind in order_centroids[i, :10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out one review for each cluster\n",
    "for i in range(10):\n",
    "    print(\"*\"*70, \"\\n\",\"Cluster: \", i+1)\n",
    "    print(df.iloc[df.groupby(['K-mean Clusters']).groups[i][0]][\"Reviews\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DBSCAN(min_samples=10) # initiate DBSCAN model\n",
    "model.fit(bow) # fit thhe model on vector form of reviews\n",
    "cluster_labels = model.labels_ # get the cluster labels\n",
    "df['DBSCAN Clusters'] = cluster_labels\n",
    "s_score = silhouette_score(bow, cluster_labels) # get and print silhouette score\n",
    "print(\"Silhouette_score of K-means clustering: \", s_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(set(cluster_labels))-1): # print clusters with keywords\n",
    "    print(\"*\"*70, \"\\n\", \"Cluster: \", i+1)\n",
    "    print(df.iloc[df.groupby(['DBSCAN Clusters']).groups[i][0]][\"Reviews\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model=gensim.models.Word2Vec(amazon_reviews) # inititate model to convert words into vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "sent_vectors = []\n",
    "for sent in amazon_reviews:\n",
    "    \"\"\"\n",
    "    This fucntion will convert each word of revews to a vector form\n",
    "    \"\"\"\n",
    "    sent_vec = np.zeros(100)\n",
    "    cnt_words =0\n",
    "    for word in sent:\n",
    "        try:\n",
    "            vec = w2v_model.wv[word]\n",
    "            sent_vec += vec\n",
    "            cnt_words += 1\n",
    "        except: pass\n",
    "    sent_vec /= cnt_words\n",
    "    sent_vectors.append(sent_vec)\n",
    "sent_vectors = np.array(sent_vectors)\n",
    "sent_vectors = np.nan_to_num(sent_vectors) # each word of a sentence as a vestor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dendro=hierarchy.dendrogram(hierarchy.linkage(sent_vectors,method='ward')) # show hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as noted above their are total 4 clusters \n",
    "cluster = AgglomerativeClustering(n_clusters=4, affinity='euclidean', linkage='ward')  #took n=4 from dendrogram curve \n",
    "Agg=cluster.fit_predict(sent_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['Hierarchical Clusters'] = cluster.labels_ # store cluster labels\n",
    "s_score = silhouette_score(bow, cluster.labels_)\n",
    "print(\"Silhouette_score of K-means clustering: \", s_score) # print silhuete score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(set(cluster.labels_))): # print clusters with reviews\n",
    "    print(\"*\"*70, \"\\n\", \"Cluster: \", i+1)\n",
    "    print(df.iloc[df.groupby(['Hierarchical Clusters']).groups[i][0]][\"Reviews\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In one paragraph, please compare K means, DBSCAN and Hierarchical clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can write you answer here. (No code needed)\n",
    "\"\"\"\n",
    "clustering method in which we make cluster s of data test(in our case the data set was text). \n",
    "K-means is most simple method provide n-number of clusters which can be small or large dependes in value of n. Although it is\n",
    "most simple and fast method but by changing number of clusters the clusters can overlap each other. but this is not in case of others\n",
    "DBSCAN is an advance method and as we saw it provide limited number of clusters, and they are not overlapping each other. but it provide more simple clusters. and they are not so unique!\n",
    "Hierarchical clustering is most advance and useful method. it has limited number of totaly independent and unique clusters with unique data sets\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
